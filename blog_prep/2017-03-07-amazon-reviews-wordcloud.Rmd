---
title: "2017-03-07-amazon-reviews-wordcloud"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## importing Amazon reviews 

### loading packages 
```{r echo = TRUE, results='hide', message=FALSE, warning=FALSE}
install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(XML, dplyr, stringr, rvest)
```


### importing Amazon reviews for Yuval Harari's "Sapiens"

```{r echo = TRUE}
# define a function removing all white space
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# define product code and the url
prod_code = "1846558239"
url <- paste0("https://www.amazon.co.uk/dp/", prod_code)
doc <- read_html(url)

#obtain the text in the node, remove "\n" from the text, and remove white space
prod <- html_nodes(doc, "#productTitle") %>% html_text() %>% gsub("\n", "", .) %>% trim()
prod
```

the web-scraping procedure and the function was described [here](https://justrthings.wordpress.com/2016/08/17/web-scraping-and-sentiment-analysis-of-amazon-reviews/)

```{r echo = TRUE}
# THE KEY: Source function to parse Amazon html pages for data
source("https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Text%20Mining/amazonscraper.R")
```


```{r echo = TRUE}
pages <- 100

reviews_all <- NULL

for(page_num in 1:pages){
  url <- paste0("http://www.amazon.co.uk/product-reviews/",prod_code,"/?pageNumber=", page_num)
  doc <- read_html(url)
  
  reviews <- amazon_scraper(doc, reviewer = F, delay = 2)
  reviews_all <- rbind(reviews_all, cbind(prod, reviews))
}

```

```{r echo = TRUE}
str(reviews_all)
```

```{r echo = TRUE}
reviews_all[1, 8]
```


### text pre-processing 

```{r echo = TRUE}
#### creating corpus 

pacman::p_load(tm, rvest, SnowballC, wordcloud)

m <- list(content = "comments")
myReader <- readTabular(mapping = m)

final_reviews <- data.frame(comments = reviews_all$comments)
ds <- DataframeSource(final_reviews)

## create corpus with all the reviews
sapiens_corpus <- VCorpus(ds)

## remove punctuation
sapiens_corpus = tm_map(sapiens_corpus, removePunctuation)

## remove numbers
sapiens_corpus = tm_map(sapiens_corpus, removeNumbers)

## LowerCase
sapiens_corpus = tm_map(sapiens_corpus, tolower)

## remove stopwords and other words
myWords=c("format", "paperback", "kindle", "edit", "hardcov", "book", "read", "will", "just", "can", "much")

sapiens_corpus <- tm_map(sapiens_corpus, removeWords, c(stopwords("english"), myWords))

## treat pre-processed documents as text documents
sapiens_corpus <- tm_map(sapiens_corpus, PlainTextDocument) 
corpus[[1]]$content

## turn into doc matrix
sapiens_dtm <- DocumentTermMatrix(sapiens_corpus)
sapiens_dtm
inspect(sapiens_dtm)
```


```{r echo = TRUE}
# displaying most frequent words
freq <- sort(colSums(as.matrix(sapiens_dtm)), decreasing=TRUE)   
head(freq, 20)  
```

### creating a wordcloud with the top 250 most frequent words

```{r echo = TRUE}
pal=brewer.pal(10, "Set1")

set.seed(100)
wordcloud(words = names(freq), freq = freq, max.words=250,
          random.order=FALSE,
          colors=pal)
```
