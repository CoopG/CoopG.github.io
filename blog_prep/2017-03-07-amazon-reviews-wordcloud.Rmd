---
title: "2017-03-07-amazon-reviews-wordcloud"
author: Kasia Kulma
output: 
  md_document:
    variant: markdown_github
---

---
layout: post
title: "This is a test post for my R blog"
date: 2016-12-01
categories: rblogging
tags: test ggplot2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the last month I discovered two things that changed my life: audiobooks and [Yuval Harari](https://en.wikipedia.org/wiki/Yuval_Noah_Harari). The former completely transformed my daily commute, the latter changed the way I think about the surrounding world (with more appreciation for history and politics, to say the least). The cocktail of the two made my brain cells sing.

Harari published two books in the last three years: [Sapiens: A Brief History of Humankind](https://www.amazon.co.uk/Sapiens-Humankind-Yuval-Noah-Harari/dp/1846558239) and more recently: [Homo Deus: A Brief History of Tomorrow](https://www.amazon.co.uk/Homo-Deus-Brief-History-Tomorrow/dp/1910701874/). Both perspective - changing, right?

My opinions aside, what do other readers think about his books? Let's dig into it by scraping Amazon reviews of "Sapiens" and then visualising their most common words in a wordcloud.

### loading packages 
```{r echo = TRUE, results='hide', message=FALSE, warning=FALSE}
install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(XML, dplyr, stringr, rvest, xml2) # web-scraping
pacman::p_load(tm, rvest, SnowballC, wordcloud) # wordcloud-building

```


### importing Amazon reviews for Yuval Harari's "Sapiens"

The web-scraping procedures that follow were shamelessly "borrowed" from [Riki Saito's blog](https://justrthings.wordpress.com/2016/08/17/web-scraping-and-sentiment-analysis-of-amazon-reviews/). Thanks, mate!

The following code requires _Amazon's product code_, which can be found in the product's URL. Next, it scrapes the product's name, just to confirm we got everything right!

```{r echo = TRUE, message=FALSE, warning=FALSE}
# define a function removing all white space
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# define product code and the url
prod_code = "1846558239"
url <- paste0("https://www.amazon.co.uk/dp/", prod_code)
doc <- xml2::read_html(url)

#obtain the text in the node, remove "\n" from the text, and remove white space
prod <- html_nodes(doc, "#productTitle") %>% html_text() %>% gsub("\n", "", .) %>% trim()
prod
```

So far, so good. Now, after sourcing `amazon_scraper` function from Riki's Github page, I import first 50 pages of reviews of _Sapiens_:

```{r echo = TRUE, message=FALSE, warning=FALSE}
# THE KEY: Source function to parse Amazon html pages for data
source("https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Text%20Mining/amazonscraper.R")
```


```{r echo = TRUE, message=FALSE, warning=FALSE}
pages <- 50

reviews_all <- NULL

for(page_num in 1:pages){
  url2 <- paste0("http://www.amazon.co.uk/product-reviews/",prod_code,"/?pageNumber=", page_num)
  doc2 <- read_html(url2)
  
  reviews <- amazon_scraper(doc2, reviewer = F, delay = 2)
  reviews_all <- rbind(reviews_all, reviews)
}
```

It looks like everything worked! The returned data frame contains not only date, title, author and content of every review, but also number of stars given, format of the book that the review is for and even the number of people that thought this particular review was helpful, priceless!

```{r echo = TRUE}
str(reviews_all)
```

In this post I'll focus on the review content only. Here's what the exemplary comment looks like:

```{r echo = TRUE}
reviews_all[1, 7]
```

As you can tell, it's not in the best shape for the analysis: it contains punctuation signs, numbers, lower and upper letters, etc. Let's sort it out by doing some text pre-processing:

### text pre-processing 

```{r echo = TRUE}
#### creating corpus 
m <- list(content = "comments")
myReader <- readTabular(mapping = m)

final_reviews <- data.frame(comments = reviews_all$comments)
ds <- DataframeSource(final_reviews)

## create corpus with all the reviews
sapiens_corpus <- VCorpus(ds)

## remove punctuation
sapiens_corpus = tm_map(sapiens_corpus, removePunctuation)

## remove numbers
sapiens_corpus = tm_map(sapiens_corpus, removeNumbers)

## LowerCase
sapiens_corpus = tm_map(sapiens_corpus, tolower)
```

I also removed some uninformative words, together with English stopwords, to make the results clearer:

```{r echo=TRUE}
## remove stopwords and other words
myWords=c("format", "paperback", "kindle", "edit", "hardcov", "book", "read", "will", "just", "can", "much")

sapiens_corpus <- tm_map(sapiens_corpus, removeWords, c(stopwords("english"), myWords))

## treat pre-processed documents as text documents
sapiens_corpus <- tm_map(sapiens_corpus, PlainTextDocument) 

## turn into doc matrix
sapiens_dtm <- DocumentTermMatrix(sapiens_corpus)
```

Let's have a quick peek into the top 20 most frequent words:

```{r echo = TRUE}
# displaying most frequent words
freq <- sort(colSums(as.matrix(sapiens_dtm)), decreasing=TRUE)   
head(freq, 20)  
```


And finally, crème de la crème, the wordcloud showing 250 most frequent words found in Amazon reviews of _Sapiens_:

### creating a wordcloud with the top 250 most frequent words

```{r echo = TRUE, warning=FALSE, message=FALSE}
pal=brewer.pal(9, "Set1")

set.seed(100)
wordcloud(words = names(freq), freq = freq, max.words=250,
          random.order=FALSE,
          colors=pal)
```

As pretty as it looks, it's not too informative regarding how positive / negative the reviews were. For this, in my next post I'll run a [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) on reviews of both books, so watch this space! 