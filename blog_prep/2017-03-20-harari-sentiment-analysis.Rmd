---
title: "2017-03-20-sentiment-analysis-of-hararis-books"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())

```
So! Following my [previous blog post](https://kkulma.github.io/2017-03-07-amazon-reviews-wordcloud/) where I scraped Amazon reviews of Yuval Harari's [_Sapiens_](https://kkulma.github.io/2017-03-07-amazon-reviews-wordcloud/) to create a wordcloud based on them, here I will compare results of [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) performed on Harari's two books: [_Sapiens_](https://en.wikipedia.org/wiki/Sapiens:_A_Brief_History_of_Humankind) and [_Deus Ex_](https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow). 

### A QUICK INTRO

For the context, _"Sapiens"_ has been published originally in Hebrew in 2011. It, as _Wikipedia_ puts it,

> [Sapiens] surveys the history of humankind from the evolution of archaic human species in the Stone Age up to the twenty-first century. 

It quickly became a bestseller, but it still took 4 years before Harari published his most recent book, and an overnight hit,  _Deus Ex_:

> Homo Deus, as opposed to the previous book, deals more with the abilities acquired by mankind (Homo sapiens) throughout the years of its existence while basing itself as the dominant being in the world, and tries to paint an image of the future of mankind, if any
_[Wikipedia]_

So, in both books the historical, philosophical, economical and biological sythesis of the human species played a big role and in this sense they are simiar. Still, _Sapiens_ was the first of its kind and it probably set high expectations for the follower book. Additionally, _Deus Ex_ makes some bold predictions about the future of human kind in the world ruled by algorithms and AI, and any such speculation will have a very polarizing effect. 


**For this reason, my prediction is that _Sapiens_ will receive more positive reviews than _Deus Ex_.**
And there's only one way to find out if I'm right, so let's get cracking!


### SENTIMENT ANALYSIS 

After loading necessary packages, I wrap up a scraping process described in my previous post [previous blog post](https://kkulma.github.io/2017-03-07-amazon-reviews-wordcloud/) into a `function_page()` function. I extract the first 13 pages of reviews for both books, to have a comparable amount of data for analysis ( _"Deus Ex"_ has been published in English only in September 2016). After some data cleaning, my data look like this: 


```{r java, echo = TRUE}
# this change is necessary to avoid Error: java.lang.OutOfMemoryError: Java heap space
# later in the process
options(java.parameters = "-Xmx3g")
```


```{r load, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
# load packages
# load packages
library(tidyverse)
library(XML)
library(xml2)
library(tidytext)
library(knitr)
library(RSentiment)
```

```{r scrape_reviews, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE}
sapiens_code = "1846558239"
deus_ex_code = "1910701874"

#Source funtion to Parse Amazon html pages for data
source("https://raw.githubusercontent.com/rjsaito/Just-R-Things/master/Text%20Mining/amazonscraper.R")

pages <- 12

function_page <- function(page_num, prod_code){
  url2 <- paste0("http://www.amazon.co.uk/product-reviews/",prod_code,"/?pageNumber=", page_num)
  doc2 <- read_html(url2)
  
  reviews <- amazon_scraper(doc2, reviewer = F, delay = 2)
  reviews
}

sapiens_reviews <- map2(1:pages, sapiens_code, function_page) %>% bind_rows()
sapiens_reviews$comments <- gsub("\\.", "\\. ", sapiens_reviews$comments)

deusex_reviews <- map2(1:pages, deus_ex_code, function_page) %>% bind_rows()
deusex_reviews$comments <- gsub("\\.", "\\. ", deusex_reviews$comments)

knitr::kable(head(deusex_reviews, 2))
```

Looks like a good start to me! Next, I'll analyse and compare word sentiments between the two books. To achieve this, I write a function that breaks down review sentences into separate words and removes common English stop words, such as _you_, _at_, _above_, etc.    

```{r sentiment_analysis, echo = TRUE, message=FALSE, error=FALSE, warning=FALSE, results='hide'}
### sentiment analysis ####

words_function <- function(df){
  df_words <- df %>% 
  select(date, comments, stars) %>% 
  unnest_tokens(word, comments)
  
  data("stop_words")
  
  df_words <- df_words %>%
    anti_join(stop_words)
  
  df_words
}

sapiens_words <- words_function(sapiens_reviews)
deusex_words <- words_function(deusex_reviews)
```

Now, there are several approches to quantifying the amount of different sentiments in text (and thus using different relevant R lexicons) : you can associate a word with a given emotion, like joy, sadness, fear etc. (**NRC lexicon**), express whether a word is positive or negative (**bing lexicon**) or give it a numeric score between -5 and 5, where values under 0 indicate a negative sentiment and above 0 - the posive. Words scoring close to or equal zero are neutral.


```{r sentiments_words, echo=TRUE}
get_sentiments("bing") %>% head
get_sentiments("nrc") %>% head
get_sentiments("afinn") %>% head
```

All I need to do now is to use `left_join()` to add both lexicons to my data:


```{r words_score, echo = TRUE}
sapiens_words <- sapiens_words %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  mutate(book = "Sapiens") %>% unique()

deusex_words <- deusex_words %>% 
  left_join(get_sentiments("bing"), by = "word") %>% 
  left_join(get_sentiments("afinn"), by = "word") %>% 
  mutate(book = "Deus Ex") %>% unique()

head(sapiens_words)
```


```{r all_words, echo = TRUE}
all_words <- bind_rows(sapiens_words, deusex_words) 
all_words %>% arrange(sentiment) %>% head() %>% kable()
```


Now we're ready to explore! For the start, star ratings should give me a flavour of the reviews' sentiments. Based on the distribution of given stars, it looks like _Deus Ex_ has more positive reviews than _Sapiens_ (convrsely to my predictions!): 83% of the reviews gave it 4 or 5 stars, whereas for _Sapiens_ it was only 52%. 

```{r no_stars, echo = TRUE}
all_words %>%
  group_by(book, stars) %>%
  summarize(n_stars = n()) %>%
  group_by(book) %>% 
  mutate(n_reviews = sum(n_stars),
         percent = paste0(round(n_stars*100/n_reviews, 0), "%")) %>% 
  select(-c(n_stars, n_reviews)) %>% 
  spread(stars, percent)

```
  
Is the same trend shown in the sentiment score?

  
```{r score_box, echo = TRUE}

all_words %>% 
  ggplot(aes(x= book, y = score, color = book, fill = book)) +
  geom_boxplot(outlier.shape=NA, alpha = 0.3) + #avoid plotting outliers twice
  scale_color_manual(values=c("#333333", "#CC0000")) +
  scale_fill_manual(values=c("#333333", "#CC0000")) 
```

Not at all! Distribution of _afnn_ sentiment score for both books looks very similar. Does it mean that the star number reflects different level of positivity in those books? Let's have a look


```{r avg_word_sent, echo = TRUE}
### average sentiment score per star 

all_words %>% 
  ggplot(aes(as.factor(stars), score)) +
  geom_boxplot(aes(fill = book), alpha = 0.3) +
  xlab("Number of stars")+ 
    scale_color_manual(values=c("#333333", "#CC0000")) +
  scale_fill_manual(values=c("#333333", "#CC0000")) 
```

Indeed, 1-star and 5-star ratings seem to be more positive for _Sapiens_ than they are for _Deus Ex_. What I find interesing is that for _Sapiens_ 1-star ratings are much more positive than 2- or 3-star ratings: their median is positive, where for the latter 2 is negative. At the same time, _Deus Ex_ shows more consistency between the sentiment score and number of stars given. Will XXX lexicon show similar patterns?


```{r echo = TRUE}
## ratio of positive / negative words per review

all_words %>%
  filter(!is.na(sentiment)) %>%
  group_by(book, sentiment) %>% 
  summarise(n = n() ) %>%
  group_by(book) %>%
  mutate(sum = sum(n),
         percent = paste0(round(n*100/sum, 0), "%")) %>%
  select(-c(n, sum)) %>%
  spread(sentiment, percent)
```

OK, so simple proportion of positive words seems slightly higher in _Sapiens_. How are these sentiments reflected in number of stars given?


```{r echo = TRUE}
### ratio of positive / negative words per star per review

all_words %>% 
  filter(!is.na(sentiment)) %>%
  group_by(book, stars, sentiment) %>%
  summarise(n = n()) %>%
  group_by(book, stars) %>%
  mutate(sum = sum(n), 
         percent = paste0(round(n*100/sum, 0), "%"),
         percent2 = round(n/sum, 3)) %>% 
  select(-c(n, sum, percent)) %>%
  spread(sentiment, percent2) %>%
  ggplot(aes(x = stars, y = positive, fill = book)) +
    geom_bar(stat = "identity", position = "identity", alpha = 0.6) +
    scale_color_manual(values=c("#333333", "#CC0000")) +
  scale_fill_manual(values=c("#333333", "#CC0000")) 

```

There you go! Weirdly enough, _Sapiens_ reviews with only 1 star are as positive as those with 5 stars! What's going on there? 


```{r 1star, echo = TRUE}

all_words %>% 
  filter(book == "Sapiens" & stars == 1, score >= 2 ) %>% 
  as.data.frame()
```



```{r pre_sentence, echo=TRUE}
#### Rsentiment and sentences ####


#### scoring sentences ####

sentence_function <- function(df){
  df_sentence <- df %>% 
    select(date, comments,  stars, helpful) %>% 
    mutate(comments = str_replace_all(comments, "[[:digit:]]", "")) %>% #remove all digits
    unnest_tokens(sentence, comments, token = "sentences") %>%
    mutate(sentence2 = str_replace_all(sentence, "[^[:alnum:]]", " "),
           clean_sentence = ifelse(grepl("[[:alpha:]]", sentence), sentence2, NA)) %>% 
    na.omit() %>%
    select(-c(sentence2)) #removing all special characters & numbers
  
  df_sentence <- df_sentence  %>%
    mutate(sentence_score = unname(calculate_score(clean_sentence))) 
  
  df_sentence
}


# go and get a hot drink while this is running 
sapiens_sentence <- sentence_function(sapiens_reviews) %>%
  mutate(book = "Sapiens") %>% unique()

deusex_sentence <- sentence_function(deusex_reviews) %>%
  mutate(book = "Deus Ex") %>% unique()

all_sentence <-bind_rows(sapiens_sentence, deusex_sentence)

knitr::kable(head(all_sentence))
```

Let's stop for a minute and have a look at the values that `RSentiment::calculate_score()` just calculated. 

```{r sentence_summary, echo = TRUE }
all_sentence %>% 
  group_by(book) %>%
  summarize(min = min(sentence_score), max = max(sentence_score), mean = mean(sentence_score),
            median = median(sentence_score))
```

They range from -7 to 7 for both books. How would the most negative and the most sentence look like?

```{r negative_sentence, echo = TRUE}
all_sentence %>%
  filter(sentence_score == -7) %>%
  as.data.frame() %>% 
  head() %>% kable()
```


```{r positive_sentence, echo = TRUE}
all_sentence %>%
  filter(sentence_score == 7) %>%
  as.data.frame() %>% 
  head() %>% kable
```
  

  
```{r avg_score_box, echo = TRUE}  
  ### adding word_count 
  
  str(all_sentence)
  
  all_sentence <- all_sentence %>%
    mutate(word_count = str_count(clean_sentence, "\\S+"),
           avg_score = round(sentence_score/word_count, 3)) %>%
    as.data.frame()
  
  
  
    ggplot(all_sentence, aes(x = book, y = sentence_score, color = book, fill = book)) +
    geom_boxplot(alpha = 0.1) +
        scale_color_manual(values=c("#333333", "#CC0000")) +
  scale_fill_manual(values=c("#333333", "#CC0000")) 
    
```  
  
```{r avg_score_star, echo = TRUE}
    all_sentence %>% 
  ggplot(aes(as.factor(stars), sentence_score)) +
  geom_boxplot(aes(fill = book), alpha = 0.3) +
  xlab("Number of stars") +
  ylab("avg_score per sentence") +
    scale_color_manual(values=c("#333333", "#CC0000")) +
  scale_fill_manual(values=c("#333333", "#CC0000")) 
```


A couple of words about `RSentiment` package: it has a huge advantage over other R packages used for text mining by analysing a sentences as a whole, so taking into account negations (e.g. _I am *not* passionate about singing_.), amplifiers (_I *really* like this song_), etc. **HOWEVER**, it's still very bug-y, e.g. returns named vectors when evaluating single sentences with `score_sentence()`, does not correctly evaluate sentences special characters) and good Lord, it is *SLOW*.

When I started working on this blog I hoped I could tell a gripping story with perhaps a bit convoluted plot but a clear and defite ending. But the reality is a bit more complex and less clear-cut: _Sapiens_ indeed seems to have slightly more positive reviews, at least in terms of proportion of positive words, compared to _Deus Ex_, and this could be due to, ironically, a more positive sentiment of most negative reviews, ha! I will leave investigation whether this is due to imperfection of the scoring method or inconsistencies of human nature to the generations of (data) scientists to come...

On the other hand, when looking at XXX sentiment score of words, or overall sentiment of sentences, there is not much difference between the two books. That's it :) Of course, there's more that can be done with this dataset: exploring whether the format of the book is associated with sentiment scores in any way, or if there's any trend in the sentiment of reviews over time. Feel free to play with those ideas if you're interested and let me know about you results!
  