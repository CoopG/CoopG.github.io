---
title: "2017-05-10-evaluating-clusters-in-unsupervised-learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the [previous post](https://kkulma.github.io/2017-04-24-determining-optimal-number-of-clusters-in-your-data/) I showed several methods that can be used to determine the optimal number of clusters in your data - this often needs to be defined for the actual clustering algorithm to run. Once it's run, however, there's no guarantee that those clusters are stable and reliable.

**In this post I'll show a couple of tests for cluster validation that can be easily run in `R`.**

Let's start!

<br> 

## VALIDATION MEASURES 

### INTERNAL MEASURES 

As the name suggests, internal validation measures rely on information in the data only, that is the characteristics of the clusters themselves, such as compactness and separation. In the perfect world we want our clusters to be as compact and separated as possible. How can this be measured?

**CONNECTIVITY**

This measure reflects the extent to which items that are placed in the same cluster are also considered their nearest neighbors in the data space - or, in other words, the degree of connectedness of the clusters. And yes, you guessed it, **it should be minimised**.

<br>

**SILHOUETTE WIDTH** 

This index defines compactness based on the pairwise distances between all elements in the cluster, and separation based on pairwise distances between all points in the cluster and all points in the closest other cluster ([Van Craenendonck & Blockeel 2015](https://lirias.kuleuven.be/bitstream/123456789/504712/1/automl_camera.pdf)) We used `silhouette` function to asses the optimal number of clusters in the previous post - and like there, **the values as close to (+) 1 as possible are mose desirable**. 

<br>

**DUNN INDEX**

Dunn Index represents the ratio of the smallest distance between observations not in the same cluster to the largest intra-cluster distance. As you can imagine, the nominator should be maximised and the denomitor minimised, **therefore the index should be maximized**.


(you can find a more detailed description of various internat cluster validation measures and their performance in this [Hui Xiong's paper](datamining.rutgers.edu/publication/internalmeasures.pdf).)

<br>

### STABILITY MEASURES 

A slightly different approach is to assess the suitability of clustering algorithm by testing how sensitive it is to perturbations in the input data. In `clValid` package this  means removing each column one at a time and re-rnning the clustering. There are several measures included, such as *average proportion of non-overlap* (APN), the *average distance* (AD),the *average distance between means* (ADM), and the *figure of merit* (FOM), all of which should be **minimised**.


Stability of clusters can be also computed using `fpc::clusterboot()` function, however the *perturbations in the input data* is a bit different here: it's done by resampling the data in a chosen way (e.g. bootstraping).


### BIOLOGICAL MEASURES 

These measures can be only applied to the narrow class of biological data, such as microarray or RNAseq data where observations correspond to genes. Essentially, biological validation evaluates the ability of a clustering algorithm to produce biologically meaningful clusters.

To find out more about all these measures, check out the `clValid` vignette by opening the  [link](https://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&cad=rja&uact=8&sqi=2&ved=0ahUKEwiSzaO5lNfTAhWKA8AKHVDRD88QFghaMAc&url=http%3A%2F%2Fhbanaszak.mjr.uw.edu.pl%2FTempTxt%2FBrockEtAl_2008_CIValidAnRPackageForClusterValidation.pdf&usg=AFQjCNGeuXuGr1_CugB1rLygXSojbs1DvQ&sig2=EkSz5W04IXOfCAUSQBLKvw)  or by typing `?clValid` in your R console. 

<br> 

## EXAMPLES 

Enough of theory! Let's have a look at the R code and some examples.

I'll rely on the scaled `wine` dataset that I used in my [previous post](https://kkulma.github.io/2017-04-24-determining-optimal-number-of-clusters-in-your-data/)). I'm not going to evaluate ALL clustering algorithms described there, but this will be enough to give you an idea how to run the cluster validation on your dataset.

### Clusterwise cluster stability assessment by resampling (fpc::clusterboot())

After preparing the data...

```{r wine, include=TRUE, message=FALSE, error=FALSE, comment=FALSE}
library(gclus)
library(ggplot2)
library(dplyr)

data(wine)
scaled_wine <- scale(wine) %>% as.data.frame()
scaled_wine2 <- scaled_wine[-1]
```

<br>

... let's validate a simple k-means algorithm using stability measures using `fpc::clusterboot()`. In my previous post, depending on the clustering method, I obtained different number of possible "best" clusters: 2,3,4 and 15. I'll now test each of these options by bootstrapping the orginal dataset 100 times:


```{r kboot, results='hide', message=FALSE, warning=FALSE}

library(fpc)

set.seed(20)
km.boot2 <- clusterboot(scaled_wine2, B=100, bootmethod="boot",
                        clustermethod=kmeansCBI,
                        krange=2, seed=20)

km.boot3 <- clusterboot(scaled_wine2, B=100, bootmethod="boot",
                        clustermethod=kmeansCBI,
                        krange=3, seed=20)

km.boot4 <- clusterboot(scaled_wine2, B=100, bootmethod="boot",
                        clustermethod=kmeansCBI,
                        krange=4, seed=20)


km.boot15 <- clusterboot(scaled_wine2, B=100, bootmethod="boot",
                        clustermethod=kmeansCBI,krange=15, seed=20)
```

<br>

Have a look at the results below. Keep in mind that 

  1) Clusterwise Jaccard bootstrap mean should be **maximised**
  2) number of dissolved clusters should be **minimised** and 
  3) number of recovered clusters should be **maximised** and as close to the number of pre-defined bootstraps as possible

```{r cboost_results, include=TRUE}
print(km.boot2)
print(km.boot3)
print(km.boot4)
print(km.boot15)
```

<br>

According to the above guidelines, it looks like 3 clusters are most stable out of all tested options. 

### Multivariate validation of cluster results (clValid package)

Now, let's validate several different clustering algorithms at the same time using internal and stability measures. `clValid` package makes it easy to compare all those measures for different clustering methods across different number of clusters (from 3 to 15). In fact, this can be done in 2 (rather long) lines of code:

```{r clvalid, include = TRUE, message=FALSE, warning=FALSE, comment=FALSE}

library(clValid)
library(kohonen)
library(mclust)

# transfrm data.frame into matrix
m_wine <- as.matrix(scaled_wine2)

valid_test <- clValid(m_wine, c(2:4, 15),
                      clMethods = c("hierarchical", "kmeans",  "pam" ),
                      validation = c("internal", "stability")
)
```

```{r summary, include = TRUE}
summary(valid_test)
```

<br> 

I love this summary! Just look at it: it not only gives you a summary of all the specified validation measures across different clustering algorithms and number of inspected clusters, but also it lists those algorithms and number of clusters pairs that performed best in regard to a given validation metric. Very helpful, especially when evaluating more algorithms and possible numbers of clusters! 

So, following the last summary, it looks like the hierarchical clustering with 2 clusters performed best in terms of stability and internal measures, as this pair appears in 2 out of 4 stability measures and 2 out of 3 internal measures.

<br> 

## BEFORE YOU GO

Coming to the end of this post, it's important to stress that there are more packages and methods you can use to evaluate your clusters (for the start, I would explore [clusteval package](ftp://ftp.u-aizu.ac.jp/pub/lang/R/CRAN/web/packages/clusteval/index.html) ), but these quick glimpses can go a long way and give you a good idea of what works for your data and what doesn't.

