---
title: "Trump VS Clinton Interpretable Text Classifier"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I've been writing/talking a lot about  [LIME](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)] recently: in [this blog](https://kkulma.github.io/2017-11-07-automated_machine_learning_in_cancer_detection/)/ at [H20 meetup](https://www.youtube.com/watch?v=CY3t11vuuOM), or at coming [AI Congress](https://theaicongress.com/) and I'm still in love with this tool for interpreting any, even black-box - algorithms! The part I love most is that LIME can be applied to both   

```{r pkgs, message=FALSE}
library(readr)
library(lime)
library(xgboost) # the classifier
library(caret)
library(dplyr)
library(tibble)
library(text2vec) 
library(qdapRegex) # removes urls from text
```


```{r data, message=FALSE}
tweets <- read_csv('data/tweets.csv')
head(tweets)
```

```{r classes}
table(tweets$handle)
```


```{r all_tweetts}
all_tweets <- tweets %>% 
  rename(author = handle) %>% 
  select(author, text) %>% 
  mutate(text = qdapRegex::rm_url(text)) %>% #removes URLs from text
  na.omit()

head(as.data.frame(all_tweets))
```

```{r model}
set.seed(1234)
trainIndex <- createDataPartition(all_tweets$author, p = .8, 
                                  list = FALSE, 
                                  times = 1)

train_tweets <- all_tweets[ trainIndex,]
test_tweets <- all_tweets[ -trainIndex,]

str(train_tweets)
```



```{r tokens, results='hide'}
# tokenizes text data nad creates Document Term Matrix
get_matrix <- function(text) {
  it <- itoken(text, progressbar = TRUE)
  create_dtm(it, vectorizer = hash_vectorizer(ngram = c(1L, 2L)))
}

dtm_train= get_matrix(train_tweets$text)
dtm_test = get_matrix(test_tweets$text)

# Extreme gradient boosting algorithm with standard parameters

param <- list(max_depth = 7, 
              eta = 0.1, 
              objective = "binary:logistic", 
              eval_metric = "error", 
              nthread = 1)

set.seed(1234)
xgb_model <- xgb.train(
  param, 
  xgb.DMatrix(dtm_train, label = train_tweets$author == "realDonaldTrump"),
  nrounds = 50,
  verbose=0
)
```


```{r predict}
# We use a (standard) threshold of 0.5
predictions <- predict(xgb_model, dtm_test) > 0.5
test_labels <- test_tweets$author == "realDonaldTrump"

# Accuracy
print(mean(predictions == test_labels))
```


```{r correct, message=FALSE}
# select only correct predictions
predictions_tb = predictions %>% as_tibble() %>% 
  rename_(predict_label = names(.)[1]) %>%
  tibble::rownames_to_column()

correct_pred = test_tweets %>%
  tibble::rownames_to_column() %>% 
  mutate(test_label = author == "realDonaldTrump") %>%
  left_join(predictions_tb) %>%
  filter(test_label == predict_label) %>% 
  pull(text) %>% 
  head(5)
```



```{r detach_dplyr, message=FALSE, error=FALSE}
detach("package:dplyr", unload=TRUE) # explainer will not run with dplyr in the workspace
```


```{r explainer}
explainer <- lime(correct_pred, model = xgb_model, 
                  preprocess = get_matrix)
```

```{r corr_explain, results='hide'}
corr_explanation <- lime::explain(correct_pred, explainer, n_labels = 1, 
                       n_features = 6, cols = 2, verbose = 0)
```


```{r features}
plot_features(corr_explanation)
```

```{r text_explanations}
plot_text_explanations(corr_explanation)
```


```{r interactive_text, results='hide', message=FALSE, error=FALSE}
# Launching the application is done in one command
interactive_text_explanations(explainer)
```



